{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c13d0d",
   "metadata": {},
   "source": [
    "# ResNet on Tiny ImageNet\n",
    "\n",
    "Residual Network is a model proposed the first time by [K.  He et al in 2015](https://arxiv.org/pdf/1512.03385.pdf), and a reviewed version in 2016 once again by [K. He et al](https://arxiv.org/pdf/1603.05027.pdf).  The idea of the ResNet, is similar to the GoogLeNet, the authors consider the idea of micro-architecture to build the macro-architecture.\n",
    "\n",
    "The micro-architecture is called of residual module. This module performs a certain number of convolution operation in parallel, considering different kernel sizes, to reduce the volume and avoid maxpooling operation. At the end of the residual module, we add the shortcut, that’s the input vector  passed at the top of the residual module, this shortcut enables the module to create a map of features. Each residual module has an associated number of filters. Using micro-architecture, the model is enable to increase the depth of the network, without increasing the running time. The head of the model is a softmax operation, to classify the images. \n",
    "\n",
    "In the compvis module, there’s two version of ResNet model, a shallow and deep version. The deep version will be used  in this example, using the Tiny ImageNet dataset. The family of ResNet is composed by several model, each model is defined by the total amount of residual module, for example, ResNet50 has 50 residual module.  We can define how many residual layers we want.\n",
    "\n",
    "To create the dataset in HDF5, click here. We consider the HDF5DatasetGenerator during the training, this reads the image on the batch size, avoiding the running out memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb44e1",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03985b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import tiny_imagenet_config as config\n",
    "from compvis.preprocessing import ImageToArrayPreprocessor\n",
    "from compvis.preprocessing import SimplePreprocessor\n",
    "from compvis.preprocessing import MeanPreprocessor\n",
    "from compvis.callbacks import TrainingMonitor\n",
    "from compvis.io import HDF5DatasetGenerator\n",
    "from compvis.nn.lr import LRFunc\n",
    "from compvis.nn.cnns import ResNet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4bed7",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a9b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the RGB mean for the normalization\n",
    "means = json.loads(open(config.DATASET_MEAN).read())\n",
    "\n",
    "# Initializing the image preprocessors\n",
    "\n",
    "sp = SimplePreprocessor(64, 64)\n",
    "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
    "iap = ImageToArrayPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28898677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the image generator (Data augumentation)\n",
    "aug = ImageDataGenerator(rotation_range=18, zoom_range=0.15, width_shift_range=0.2,\n",
    "                         height_shift_range=0.2, shear_range=0.15, horizontal_flip=True,\n",
    "                         fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6746da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the training and validation into HDF5 file\n",
    "trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, 64, aug=aug, \n",
    "                                preprocessors=[sp, mp, iap], \n",
    "                                classes=config.NUM_CLASSES)\n",
    "valGen = HDF5DatasetGenerator(config.VAL_HDF5, 64, preprocessors=[sp, mp, iap],\n",
    "                              classes=config.NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1aed1",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "To build the model, we consider the class ResNet. Before define and training the model, we need to set some regularization and, define the learning rate scheduler that will be used during the training. \n",
    "\n",
    "**Learning rate polynomial function decay**\n",
    "\n",
    "During the training, the learning rate will be dropped down, at each step, we consider a polynomial function to decrease the learning rate.\n",
    "\n",
    "We consider the class LRFunc and your attribute, poly_decay. When using the poly_decay, we must pass some arguments as initial learning rate, total number of epochs and the power of the polynomial function, in our training we consider a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f675a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-1\n",
    "epochs = 75\n",
    "deg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4147fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = LRFunc(l_r = init_lr, epochs = epochs, degree = deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b3198",
   "metadata": {},
   "source": [
    "**Defining Callbacks**\n",
    "\n",
    "Using the callback class from TensorFlow, we can consider the Training Monitor (print out of the learning curves) and the LearningRateScheduler, to reduce the learning rate using the poly_decay attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e3dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the set of callbacks\n",
    "figPath = os.path.sep.join(['output', \"{}.png\".format(os.getpid())])\n",
    "jsonPath = os.path.sep.join(['output', \"{}.json\".format(os.getpid())])\n",
    "callbacks = [TrainingMonitor(figPath, jsonPath=jsonPath), \n",
    "             LearningRateScheduler(lrs.poly_decay)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1bf2a",
   "metadata": {},
   "source": [
    "**Building the model**\n",
    "\n",
    "Using the class ResNet, we have the attribute build, that requires some arguments as, width, height, depth, number of class, stage, filters and regularization value.\n",
    "\n",
    "The stages are $(3, 4, 6)$ and the filters $(64, 128, 256, 512)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76194a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] compiling model...\")\n",
    "model = ResNet.build(64, 64, 3, config.NUM_CLASSES, config.stage, config.filters, \n",
    "                     reg=0.0005, dataset=\"tiny_imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b24a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=init_lr, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954c9fa",
   "metadata": {},
   "source": [
    "**Training the model**\n",
    "\n",
    "We train the model over $75$ epochs and batch size of $64$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a8bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1406 steps, validate for 156 steps\n",
      "\n",
      "\n",
      "The parameters for the Polynomial Decay function:\n",
      "- Initial learning rate: 0.100000 \n",
      "- Epochs: 75\n",
      "- Degree: 1.\n",
      "\n",
      "Epoch 1/75\n",
      "1406/1406 [==============================] - 169s 120ms/step - loss: 5.2443 - accuracy: 0.0586 - val_loss: 4.9122 - val_accuracy: 0.0867\n",
      "Epoch 2/75\n",
      "1406/1406 [==============================] - 165s 118ms/step - loss: 4.5590 - accuracy: 0.1275 - val_loss: 4.6370 - val_accuracy: 0.1241\n",
      "Epoch 3/75\n",
      "1406/1406 [==============================] - 165s 117ms/step - loss: 4.2733 - accuracy: 0.1708 - val_loss: 4.7627 - val_accuracy: 0.1365\n",
      "Epoch 4/75\n",
      "1406/1406 [==============================] - 163s 116ms/step - loss: 4.1044 - accuracy: 0.2003 - val_loss: 4.5985 - val_accuracy: 0.1705\n",
      "Epoch 5/75\n",
      "1406/1406 [==============================] - 163s 116ms/step - loss: 3.9956 - accuracy: 0.2262 - val_loss: 4.8224 - val_accuracy: 0.1625\n",
      "Epoch 6/75\n",
      "1406/1406 [==============================] - 168s 120ms/step - loss: 3.9101 - accuracy: 0.2477 - val_loss: 4.8368 - val_accuracy: 0.1753\n",
      "Epoch 7/75\n",
      "1406/1406 [==============================] - 165s 117ms/step - loss: 3.8591 - accuracy: 0.2621 - val_loss: 4.9326 - val_accuracy: 0.1847\n",
      "Epoch 8/75\n",
      "1406/1406 [==============================] - 171s 121ms/step - loss: 3.8161 - accuracy: 0.2766 - val_loss: 5.4537 - val_accuracy: 0.1432\n",
      "Epoch 9/75\n",
      "1406/1406 [==============================] - 173s 123ms/step - loss: 3.7758 - accuracy: 0.2897 - val_loss: 4.7329 - val_accuracy: 0.1893\n",
      "Epoch 10/75\n",
      "1406/1406 [==============================] - 170s 121ms/step - loss: 3.7457 - accuracy: 0.2998 - val_loss: 4.4492 - val_accuracy: 0.2283\n",
      "Currently learning rate  0.086667\n",
      "Epoch 11/75\n",
      "1406/1406 [==============================] - 169s 120ms/step - loss: 3.7195 - accuracy: 0.3105 - val_loss: 4.5983 - val_accuracy: 0.2094\n",
      "Epoch 12/75\n",
      "1406/1406 [==============================] - 165s 118ms/step - loss: 3.6924 - accuracy: 0.3194 - val_loss: 5.0192 - val_accuracy: 0.1880\n",
      "Epoch 13/75\n",
      "1406/1406 [==============================] - 163s 116ms/step - loss: 3.6773 - accuracy: 0.3271 - val_loss: 4.3208 - val_accuracy: 0.2509\n",
      "Epoch 14/75\n",
      "1406/1406 [==============================] - 161s 115ms/step - loss: 3.6477 - accuracy: 0.3366 - val_loss: 4.4866 - val_accuracy: 0.2426\n",
      "Epoch 15/75\n",
      "1406/1406 [==============================] - 162s 115ms/step - loss: 3.6270 - accuracy: 0.3433 - val_loss: 4.2198 - val_accuracy: 0.2608\n",
      "Epoch 16/75\n",
      "1406/1406 [==============================] - 163s 116ms/step - loss: 3.6034 - accuracy: 0.3509 - val_loss: 4.2790 - val_accuracy: 0.2611\n",
      "Epoch 17/75\n",
      "1406/1406 [==============================] - 164s 116ms/step - loss: 3.5776 - accuracy: 0.3586 - val_loss: 4.1006 - val_accuracy: 0.2961\n",
      "Epoch 18/75\n",
      "1406/1406 [==============================] - 162s 115ms/step - loss: 3.5565 - accuracy: 0.3683 - val_loss: 4.2486 - val_accuracy: 0.2698\n",
      "Epoch 19/75\n",
      "1406/1406 [==============================] - 161s 115ms/step - loss: 3.5374 - accuracy: 0.3740 - val_loss: 4.3728 - val_accuracy: 0.2611\n",
      "Epoch 20/75\n",
      "1406/1406 [==============================] - 161s 114ms/step - loss: 3.5174 - accuracy: 0.3796 - val_loss: 4.1850 - val_accuracy: 0.2830\n",
      "Currently learning rate  0.073333\n",
      "Epoch 21/75\n",
      "1406/1406 [==============================] - 161s 114ms/step - loss: 3.4982 - accuracy: 0.3871 - val_loss: 4.1386 - val_accuracy: 0.2910\n",
      "Epoch 22/75\n",
      "1406/1406 [==============================] - 160s 114ms/step - loss: 3.4711 - accuracy: 0.3925 - val_loss: 4.2132 - val_accuracy: 0.2834\n",
      "Epoch 23/75\n",
      "1406/1406 [==============================] - 160s 114ms/step - loss: 3.4569 - accuracy: 0.3969 - val_loss: 3.9550 - val_accuracy: 0.3260\n",
      "Epoch 24/75\n",
      "1406/1406 [==============================] - 160s 114ms/step - loss: 3.4410 - accuracy: 0.4042 - val_loss: 4.2026 - val_accuracy: 0.2974\n",
      "Epoch 25/75\n",
      "1406/1406 [==============================] - 160s 114ms/step - loss: 3.4181 - accuracy: 0.4078 - val_loss: 4.0389 - val_accuracy: 0.3148\n",
      "Epoch 26/75\n",
      "1406/1406 [==============================] - 160s 114ms/step - loss: 3.3996 - accuracy: 0.4150 - val_loss: 3.8814 - val_accuracy: 0.3444\n",
      "Epoch 27/75\n",
      "1406/1406 [==============================] - 160s 114ms/step - loss: 3.3818 - accuracy: 0.4200 - val_loss: 4.1612 - val_accuracy: 0.3098\n",
      "Epoch 28/75\n",
      "1406/1406 [==============================] - 160s 114ms/step - loss: 3.3555 - accuracy: 0.4255 - val_loss: 4.2784 - val_accuracy: 0.3018\n",
      "Epoch 29/75\n",
      "1406/1406 [==============================] - 167s 119ms/step - loss: 3.3460 - accuracy: 0.4302 - val_loss: 4.0287 - val_accuracy: 0.3308\n",
      "Epoch 30/75\n",
      "1406/1406 [==============================] - 171s 122ms/step - loss: 3.3225 - accuracy: 0.4350 - val_loss: 4.1723 - val_accuracy: 0.3157\n",
      "Currently learning rate  0.060000\n",
      "Epoch 31/75\n",
      "1406/1406 [==============================] - 172s 122ms/step - loss: 3.3035 - accuracy: 0.4404 - val_loss: 4.2465 - val_accuracy: 0.3114\n",
      "Epoch 32/75\n",
      "1406/1406 [==============================] - 172s 122ms/step - loss: 3.2811 - accuracy: 0.4430 - val_loss: 4.3948 - val_accuracy: 0.2993\n",
      "Epoch 33/75\n",
      "1406/1406 [==============================] - 170s 121ms/step - loss: 3.2624 - accuracy: 0.4503 - val_loss: 4.2870 - val_accuracy: 0.3136\n",
      "Epoch 34/75\n",
      "1406/1406 [==============================] - 168s 119ms/step - loss: 3.2361 - accuracy: 0.4545 - val_loss: 4.0295 - val_accuracy: 0.3504\n",
      "Epoch 35/75\n",
      "1406/1406 [==============================] - 168s 120ms/step - loss: 3.2224 - accuracy: 0.4573 - val_loss: 4.3624 - val_accuracy: 0.3161\n",
      "Epoch 36/75\n",
      "1406/1406 [==============================] - 169s 120ms/step - loss: 3.1976 - accuracy: 0.4642 - val_loss: 4.4451 - val_accuracy: 0.3300\n",
      "Epoch 37/75\n",
      "1406/1406 [==============================] - 172s 122ms/step - loss: 3.1789 - accuracy: 0.4683 - val_loss: 4.2476 - val_accuracy: 0.3372\n",
      "Epoch 38/75\n",
      "1406/1406 [==============================] - 168s 120ms/step - loss: 3.1575 - accuracy: 0.4727 - val_loss: 4.1944 - val_accuracy: 0.3244\n",
      "Epoch 39/75\n",
      "1406/1406 [==============================] - 167s 119ms/step - loss: 3.1286 - accuracy: 0.4796 - val_loss: 4.3752 - val_accuracy: 0.3347\n",
      "Epoch 40/75\n",
      "1406/1406 [==============================] - 168s 119ms/step - loss: 3.1049 - accuracy: 0.4844 - val_loss: 4.0132 - val_accuracy: 0.3656\n",
      "Currently learning rate  0.046667\n",
      "Epoch 41/75\n",
      "1406/1406 [==============================] - 170s 121ms/step - loss: 3.0904 - accuracy: 0.4872 - val_loss: 4.4068 - val_accuracy: 0.3277\n",
      "Epoch 42/75\n",
      "1406/1406 [==============================] - 173s 123ms/step - loss: 3.0665 - accuracy: 0.4934 - val_loss: 3.9565 - val_accuracy: 0.3786\n",
      "Epoch 43/75\n",
      "1406/1406 [==============================] - 177s 126ms/step - loss: 3.0359 - accuracy: 0.4994 - val_loss: 4.3127 - val_accuracy: 0.3246\n",
      "Epoch 44/75\n",
      "1406/1406 [==============================] - 175s 124ms/step - loss: 3.0107 - accuracy: 0.5034 - val_loss: 3.9203 - val_accuracy: 0.3841\n",
      "Epoch 45/75\n",
      "1406/1406 [==============================] - 165s 118ms/step - loss: 2.9814 - accuracy: 0.5077 - val_loss: 4.1515 - val_accuracy: 0.3618\n",
      "Epoch 46/75\n",
      "1406/1406 [==============================] - 165s 118ms/step - loss: 2.9605 - accuracy: 0.5137 - val_loss: 4.2194 - val_accuracy: 0.3639\n",
      "Epoch 47/75\n",
      "1406/1406 [==============================] - 164s 117ms/step - loss: 2.9401 - accuracy: 0.5169 - val_loss: 3.7068 - val_accuracy: 0.4131\n",
      "Epoch 48/75\n",
      "1406/1406 [==============================] - 166s 118ms/step - loss: 2.8989 - accuracy: 0.5245 - val_loss: 4.1081 - val_accuracy: 0.3715\n",
      "Epoch 49/75\n",
      "1061/1406 [=====================>........] - ETA: 39s - loss: 2.8794 - accuracy: 0.5298"
     ]
    }
   ],
   "source": [
    "# training the model with the batch size 64\n",
    "model.fit(trainGen.generator(),steps_per_epoch=trainGen.numImages // 64,\n",
    "                    validation_data=valGen.generator(),validation_steps=valGen.numImages // 64, \n",
    "                    epochs=epochs, max_queue_size=64 * 2, callbacks=callbacks, workers=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('output/resnet_tiny.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0294c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen.close()\n",
    "valGen.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb11ea",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've trained a deeper ResNet architecture on Tiny ImageNet. We obtained a good accuracy on the validation set of $0.4913$. The evaluation of the model can be found in this [file](https://github.com/IgorMeloS/Computer-Vision-Training/blob/main/Practical%20Examples/12%20-%20ResNet/evaluating.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
