{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb10599",
   "metadata": {},
   "source": [
    "# ResNet on CIFAR10\n",
    "\n",
    "Residual Network is a model proposed the first time by [K.  He et al in 2015](https://arxiv.org/pdf/1512.03385.pdf), and a reviewed version in 2016 once again by [K. He et al](https://arxiv.org/pdf/1603.05027.pdf).  The idea of the ResNet, is similar to the GoogLeNet, the authors consider the idea of micro-architecture to build the macro-architecture.\n",
    "\n",
    "The micro-architecture is called of residual module. This module performs a certain number of convolution operation, considering different kernel sizes, to reduce the volume and avoid maxpooling operation. At the end of the residual module, we add the shortcut, that’s the input vector  passed at the top of the residual module, this shortcut enables the module to create a map of features. Each residual module has an associated number of filters. Using micro-architecture, the model is enable to increase the depth of the network, without increasing the running time. The head of the model is a softmax operation, to classify the images. \n",
    "\n",
    "In the compvis module, there’s two version of ResNet model, a shallow and deep version. The shallow version will be used  in this example, using the CIFAR10 dataset. The family of ResNet is composed by several model, each model is defined by the total amount of residual module, for example, ResNet50 has 50 residual module.  We can define how many residual layers we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b0be8",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046360d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from compvis.nn.cnns import ResNet\n",
    "from compvis.callbacks import TrainingMonitor\n",
    "from compvis.nn.lr import LRFunc\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.setrecursionlimit(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda0970",
   "metadata": {},
   "source": [
    "## Loading and preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81728f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and splitting the dataset\n",
    "((X_train, y_train), (X_test, y_test)) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52db9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing with mean substraction\n",
    "mean = np.mean(X_train, axis = 0)\n",
    "X_train -= mean\n",
    "X_test -= mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b476733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the labels into numercial vectors\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256fc4c6",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "To build the model, we consider the class ResNet. Before define and training the model, we need to set some regularization and, define the learning rate scheduler that will be used during the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a2858",
   "metadata": {},
   "source": [
    "**Learning rate polynomial function decay**\n",
    "\n",
    "During the training, the learning rate will be dropped down, at each step, we consider a polynomial function to decrease the learning rate.\n",
    "\n",
    "We consider the class LRFunc and your attribute, poly_decay. When using the poly_decay, we must pass some arguments as initial learning rate, total number of epochs and the power of the polynomial function, in our training we consider a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc14caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-1\n",
    "epochs = 100\n",
    "deg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28871e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = LRFunc(l_r = init_lr, epochs = epochs, degree=deg) # defining the LRFunc class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3f441",
   "metadata": {},
   "source": [
    "**Image augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787f12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Data augumentation to avoid the overfit\n",
    "aug = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e1b39",
   "metadata": {},
   "source": [
    "**Defining Callbacks**\n",
    "\n",
    "Using the callback class from TensorFlow, we can consider the Training Monitor (print out of the learning curves) and the LearningRateScheduler, to reduce the learning rate using the poly_decay attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e278bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to store the history of learning curves and their graphics\n",
    "figPath = os.path.sep.join([\"/output\", \"{}.png\".format(os.getpid())])\n",
    "jsonPath = os.path.sep.join([\"/output\", \"{}.json\".format(os.getpid())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56390227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the callbacks\n",
    "callbacks = [TrainingMonitor(figPath, jsonPath=jsonPath),\n",
    "             LearningRateScheduler(lrs.poly_decay)]#we consider the attribute poly_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613828df",
   "metadata": {},
   "source": [
    "**Building the model**\n",
    "\n",
    "Using the class ResNet, we have the attribute build, that requires some arguments as, width, height, depth, number of class, stage, filters and regularization value.\n",
    "\n",
    "The argument stage is a tuple of int values. These values correspond to the total of inception modules, it means, if we have $9$ as value, the model will stack $9$ residual modules, building a block of residual. The argument filter is another tuple of int values. The values correspond to the total number of filters in the convolution process. The first value in the tuple does not have correspondence with the residual modules, this value will be applied on the first convolution operation before starting the residual blocks. \n",
    "\n",
    "For example, when we pass $9$ as the first value of the stage tuple, the convolutional process will have $64$ filter, that is the second value in the filters tuple. When we pass 9 the second value in the stage tuple, the correspondent in the filters tuple will be $128$ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "101f5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = (9, 9, 9)\n",
    "filters = (64, 64, 128, 256)\n",
    "nclass = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc1591ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model and the optimizer\n",
    "opt = SGD(lr=init_lr, momentum=0.9)\n",
    "model = ResNet.build(32, 32, 3, nclass, stage, filters, reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c578fe3",
   "metadata": {},
   "source": [
    "**Training the model**\n",
    "\n",
    "We train the model over $100$ epochs and batch size of $128$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4492bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 390 steps, validate on 10000 samples\n",
      "\n",
      "\n",
      "The parameters for the Polynomial Decay function:\n",
      "- Initial learning rate: 0.100000 \n",
      "- Epochs: 100\n",
      "- Degree: 1.\n",
      "\n",
      "Epoch 1/100\n",
      "390/390 [==============================] - 92s 236ms/step - loss: 2.1904 - accuracy: 0.4121 - val_loss: 2.1631 - val_accuracy: 0.4579\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 1.6437 - accuracy: 0.5933 - val_loss: 1.6548 - val_accuracy: 0.5995\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 88s 226ms/step - loss: 1.3537 - accuracy: 0.6866 - val_loss: 1.5536 - val_accuracy: 0.6284\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 88s 224ms/step - loss: 1.1716 - accuracy: 0.7392 - val_loss: 1.2205 - val_accuracy: 0.7249\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 85s 219ms/step - loss: 1.0645 - accuracy: 0.7672 - val_loss: 1.1604 - val_accuracy: 0.7395\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 87s 222ms/step - loss: 0.9835 - accuracy: 0.7929 - val_loss: 1.0619 - val_accuracy: 0.7687\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 87s 222ms/step - loss: 0.9281 - accuracy: 0.8055 - val_loss: 1.0297 - val_accuracy: 0.7737\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 85s 218ms/step - loss: 0.8691 - accuracy: 0.8202 - val_loss: 0.8998 - val_accuracy: 0.8140\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.8355 - accuracy: 0.8305 - val_loss: 0.8641 - val_accuracy: 0.8149\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.8024 - accuracy: 0.8353 - val_loss: 0.8973 - val_accuracy: 0.8116\n",
      "Currently learning rate  0.090000\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.7715 - accuracy: 0.8456 - val_loss: 0.8474 - val_accuracy: 0.8259\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 84s 216ms/step - loss: 0.7442 - accuracy: 0.8517 - val_loss: 0.8969 - val_accuracy: 0.8019\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 86s 219ms/step - loss: 0.7213 - accuracy: 0.8569 - val_loss: 0.8615 - val_accuracy: 0.8213\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 86s 221ms/step - loss: 0.7157 - accuracy: 0.8564 - val_loss: 0.8458 - val_accuracy: 0.8166\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 87s 223ms/step - loss: 0.6862 - accuracy: 0.8680 - val_loss: 0.7806 - val_accuracy: 0.8388\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 87s 222ms/step - loss: 0.6762 - accuracy: 0.8687 - val_loss: 0.7779 - val_accuracy: 0.8340\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 86s 221ms/step - loss: 0.6622 - accuracy: 0.8714 - val_loss: 0.8587 - val_accuracy: 0.8177\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 87s 222ms/step - loss: 0.6481 - accuracy: 0.8760 - val_loss: 0.8675 - val_accuracy: 0.8060\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 85s 218ms/step - loss: 0.6368 - accuracy: 0.8789 - val_loss: 0.7673 - val_accuracy: 0.8343\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 86s 220ms/step - loss: 0.6239 - accuracy: 0.8816 - val_loss: 0.7056 - val_accuracy: 0.8538\n",
      "Currently learning rate  0.080000\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 85s 218ms/step - loss: 0.6128 - accuracy: 0.8862 - val_loss: 0.7767 - val_accuracy: 0.8374\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 86s 220ms/step - loss: 0.6063 - accuracy: 0.8881 - val_loss: 0.7914 - val_accuracy: 0.8339\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 86s 220ms/step - loss: 0.6033 - accuracy: 0.8871 - val_loss: 0.7016 - val_accuracy: 0.8618\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 85s 218ms/step - loss: 0.5954 - accuracy: 0.8894 - val_loss: 0.7600 - val_accuracy: 0.8455\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 86s 221ms/step - loss: 0.5815 - accuracy: 0.8943 - val_loss: 0.7964 - val_accuracy: 0.8273\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 87s 222ms/step - loss: 0.5788 - accuracy: 0.8935 - val_loss: 0.7064 - val_accuracy: 0.8566\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 87s 223ms/step - loss: 0.5721 - accuracy: 0.8954 - val_loss: 0.6753 - val_accuracy: 0.8670\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 86s 222ms/step - loss: 0.5646 - accuracy: 0.8997 - val_loss: 0.7716 - val_accuracy: 0.8344\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 86s 220ms/step - loss: 0.5539 - accuracy: 0.9005 - val_loss: 0.7261 - val_accuracy: 0.8514\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 86s 220ms/step - loss: 0.5517 - accuracy: 0.9022 - val_loss: 0.6880 - val_accuracy: 0.8646\n",
      "Currently learning rate  0.070000\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 88s 225ms/step - loss: 0.5473 - accuracy: 0.9052 - val_loss: 0.6934 - val_accuracy: 0.8562\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 87s 223ms/step - loss: 0.5333 - accuracy: 0.9081 - val_loss: 0.6780 - val_accuracy: 0.8674\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 85s 219ms/step - loss: 0.5357 - accuracy: 0.9072 - val_loss: 0.7346 - val_accuracy: 0.8509\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 86s 221ms/step - loss: 0.5217 - accuracy: 0.9116 - val_loss: 0.6901 - val_accuracy: 0.8580\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 86s 220ms/step - loss: 0.5180 - accuracy: 0.9140 - val_loss: 0.6215 - val_accuracy: 0.8814\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 86s 220ms/step - loss: 0.5135 - accuracy: 0.9144 - val_loss: 0.6421 - val_accuracy: 0.8800\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 85s 217ms/step - loss: 0.5114 - accuracy: 0.9132 - val_loss: 0.6635 - val_accuracy: 0.8693\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.5035 - accuracy: 0.9168 - val_loss: 0.6553 - val_accuracy: 0.8733\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.4954 - accuracy: 0.9196 - val_loss: 0.6522 - val_accuracy: 0.8749\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 84s 216ms/step - loss: 0.4996 - accuracy: 0.9171 - val_loss: 0.6624 - val_accuracy: 0.8690\n",
      "Currently learning rate  0.060000\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.4917 - accuracy: 0.9199 - val_loss: 0.6335 - val_accuracy: 0.8741\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.4803 - accuracy: 0.9241 - val_loss: 0.6437 - val_accuracy: 0.8758\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.4802 - accuracy: 0.9230 - val_loss: 0.6247 - val_accuracy: 0.8802\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.4754 - accuracy: 0.9242 - val_loss: 0.6036 - val_accuracy: 0.8882\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4709 - accuracy: 0.9254 - val_loss: 0.5985 - val_accuracy: 0.8875\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4586 - accuracy: 0.9296 - val_loss: 0.5988 - val_accuracy: 0.8845\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4545 - accuracy: 0.9305 - val_loss: 0.6834 - val_accuracy: 0.8680\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4502 - accuracy: 0.9321 - val_loss: 0.6030 - val_accuracy: 0.8867\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4468 - accuracy: 0.9332 - val_loss: 0.6234 - val_accuracy: 0.8799\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4385 - accuracy: 0.9331 - val_loss: 0.5989 - val_accuracy: 0.8872\n",
      "Currently learning rate  0.050000\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4374 - accuracy: 0.9351 - val_loss: 0.5641 - val_accuracy: 0.8971\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4275 - accuracy: 0.9381 - val_loss: 0.6359 - val_accuracy: 0.8775\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4274 - accuracy: 0.9370 - val_loss: 0.7050 - val_accuracy: 0.8631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4251 - accuracy: 0.9371 - val_loss: 0.6067 - val_accuracy: 0.8809\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4200 - accuracy: 0.9395 - val_loss: 0.6312 - val_accuracy: 0.8807\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 83s 212ms/step - loss: 0.4085 - accuracy: 0.9426 - val_loss: 0.6537 - val_accuracy: 0.8763\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.4063 - accuracy: 0.9428 - val_loss: 0.6129 - val_accuracy: 0.8838\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3968 - accuracy: 0.9451 - val_loss: 0.6075 - val_accuracy: 0.8852\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3935 - accuracy: 0.9451 - val_loss: 0.5447 - val_accuracy: 0.8983\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3925 - accuracy: 0.9470 - val_loss: 0.5545 - val_accuracy: 0.8970\n",
      "Currently learning rate  0.040000\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3764 - accuracy: 0.9513 - val_loss: 0.5731 - val_accuracy: 0.8952\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3753 - accuracy: 0.9515 - val_loss: 0.5972 - val_accuracy: 0.8928\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3681 - accuracy: 0.9527 - val_loss: 0.5798 - val_accuracy: 0.8948\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 83s 212ms/step - loss: 0.3696 - accuracy: 0.9525 - val_loss: 0.5729 - val_accuracy: 0.8934\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3550 - accuracy: 0.9564 - val_loss: 0.6142 - val_accuracy: 0.8830\n",
      "Epoch 66/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3517 - accuracy: 0.9559 - val_loss: 0.5562 - val_accuracy: 0.8996\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3488 - accuracy: 0.9563 - val_loss: 0.5548 - val_accuracy: 0.8976\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3415 - accuracy: 0.9591 - val_loss: 0.5302 - val_accuracy: 0.9052\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3364 - accuracy: 0.9595 - val_loss: 0.5770 - val_accuracy: 0.8962\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3271 - accuracy: 0.9618 - val_loss: 0.5363 - val_accuracy: 0.9000\n",
      "Currently learning rate  0.030000\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3225 - accuracy: 0.9627 - val_loss: 0.5801 - val_accuracy: 0.8952\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 83s 212ms/step - loss: 0.3163 - accuracy: 0.9651 - val_loss: 0.5558 - val_accuracy: 0.9003\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3099 - accuracy: 0.9667 - val_loss: 0.5379 - val_accuracy: 0.9028\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3024 - accuracy: 0.9687 - val_loss: 0.5108 - val_accuracy: 0.9083\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2997 - accuracy: 0.9686 - val_loss: 0.5325 - val_accuracy: 0.9035\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2872 - accuracy: 0.9721 - val_loss: 0.5190 - val_accuracy: 0.9096\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2791 - accuracy: 0.9736 - val_loss: 0.5442 - val_accuracy: 0.9032\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2722 - accuracy: 0.9750 - val_loss: 0.5821 - val_accuracy: 0.8962\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2668 - accuracy: 0.9762 - val_loss: 0.5304 - val_accuracy: 0.9095\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2612 - accuracy: 0.9778 - val_loss: 0.5111 - val_accuracy: 0.9132\n",
      "Currently learning rate  0.020000\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2517 - accuracy: 0.9797 - val_loss: 0.5209 - val_accuracy: 0.9107\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2480 - accuracy: 0.9806 - val_loss: 0.5241 - val_accuracy: 0.9104\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2360 - accuracy: 0.9840 - val_loss: 0.5036 - val_accuracy: 0.9123\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2345 - accuracy: 0.9834 - val_loss: 0.4947 - val_accuracy: 0.9162\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2225 - accuracy: 0.9868 - val_loss: 0.4783 - val_accuracy: 0.9171\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2172 - accuracy: 0.9874 - val_loss: 0.5202 - val_accuracy: 0.9125\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2132 - accuracy: 0.9885 - val_loss: 0.4783 - val_accuracy: 0.9208\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2033 - accuracy: 0.9910 - val_loss: 0.4805 - val_accuracy: 0.9178\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1972 - accuracy: 0.9924 - val_loss: 0.4895 - val_accuracy: 0.9173\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1898 - accuracy: 0.9941 - val_loss: 0.4763 - val_accuracy: 0.9229\n",
      "Currently learning rate  0.010000\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1863 - accuracy: 0.9945 - val_loss: 0.4761 - val_accuracy: 0.9221\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1811 - accuracy: 0.9958 - val_loss: 0.4462 - val_accuracy: 0.9287\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1768 - accuracy: 0.9966 - val_loss: 0.4598 - val_accuracy: 0.9250\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1723 - accuracy: 0.9972 - val_loss: 0.4439 - val_accuracy: 0.9250\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1691 - accuracy: 0.9979 - val_loss: 0.4356 - val_accuracy: 0.9297\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 83s 212ms/step - loss: 0.1671 - accuracy: 0.9981 - val_loss: 0.4449 - val_accuracy: 0.9277\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1655 - accuracy: 0.9982 - val_loss: 0.4427 - val_accuracy: 0.9293\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1628 - accuracy: 0.9989 - val_loss: 0.4331 - val_accuracy: 0.9315\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1617 - accuracy: 0.9989 - val_loss: 0.4290 - val_accuracy: 0.9306\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1607 - accuracy: 0.9992 - val_loss: 0.4286 - val_accuracy: 0.9297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea1584a090>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(aug.flow(X_train, y_train, batch_size=128), validation_data=(X_test, y_test),\n",
    "          steps_per_epoch=len(X_train) // 128, epochs=epochs, \n",
    "          callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "891654cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('output/model_lr1.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c64e11",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The results after the training are interesting, we've obtained $0.9297$ as accuracy on the validation set. On the other hand, if we look with attention, the over-fit persists on the CIFAR10 dataset, maybe to obtain a better results, we must use transfer learning. Until now, in this training repository, this is the best result on the validation set, that's a good result for a network trained from scratch. The graphic with the results is found here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
