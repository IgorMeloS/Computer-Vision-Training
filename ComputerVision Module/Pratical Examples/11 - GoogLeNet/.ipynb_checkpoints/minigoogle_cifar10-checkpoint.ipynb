{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dbcfff1",
   "metadata": {},
   "source": [
    "# Mini GoogLeNet on CIFAR-10\n",
    "\n",
    "GoogLeNet was proposed in 2014 by [Szegedy et al.](https://arxiv.org/pdf/1409.4842.pdf) This Convolutional Neural Network (CNN) has introduced the concept of micro-architecture, it means, the model is composed by a certain number of micro-architecture, forming the macro-architecture.\n",
    "\n",
    "GoogLeNet introduced the inception module, it's composed by three convolution processing, including kernels size of $(1x1)$, $(3x3)$ and $(5x5)$. Each of them is parallel to the others during the running. The model was capable to increase the depth of the CNN, conserving a reasonable running time. At the end of the inception module, the model down sample all information to put into a feature map. If there's other inception module, other convolutions are performed, otherwise there's a maxpooling process and, the feature map is connected into the fully-connected layer, to make predictions. This model won the ImageNet Large-Scale Visual Recognition Challenge 2014.\n",
    "\n",
    "In this example, we consider a reduced form of GoogLeNet, we implement the Mini GoogLeNet on the CIFAR-10 dataset. The Mini GoogLeNet, considers less convolutions layers and, the inception module realizes just two convolution process $(1x1)$ and $(3x3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cbd8d9",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5891b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from compvis.nn.cnns import MiniGoogLeNet\n",
    "from compvis.callbacks import TrainingMonitor\n",
    "from compvis.nn.lr import LRFunc\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecdbe2b",
   "metadata": {},
   "source": [
    "## Loading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34dbf7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading the CIFAR-10 dataset ...\n"
     ]
    }
   ],
   "source": [
    "# Loading and splitting the dataset\n",
    "((X_train, y_train), (X_test, y_test)) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ca384",
   "metadata": {},
   "source": [
    "**Mean substraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07581d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing with mean substraction\n",
    "mean = np.mean(X_train, axis = 0)\n",
    "X_train -= mean\n",
    "X_test -= mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ad148",
   "metadata": {},
   "source": [
    "**Encoding labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa8b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the labels into numercial vectors\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf8eed",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train the model, we consider $70$ epochs. During the training, the learning rate drops down, following a polynomial function decay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36fd7f3",
   "metadata": {},
   "source": [
    "**Regularizations**\n",
    "\n",
    "We consider the class LRFunctions, this class offers some learning rate functions to be used with LearningRateScheduler. The required arguments for this example are l_r (initial learning rate), epochs (number of epochs) and degree (the function degree, in this case linear).\n",
    "\n",
    "We also consider the image augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9baf1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = LRFunc(l_r = 0.001, epochs = 70, degree=1) # defining the LRFunc class\n",
    "\n",
    "# Defining the Data augumentation to avoid the overfit\n",
    "aug = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# Building the set of callbacks\n",
    "\n",
    "figPath = os.path.sep.join([\"/path/to/output\", \"{}.png\".format(os.getpid())])\n",
    "jsonPath = os.path.sep.join([\"/path/to/output\", \"{}.json\".format(os.getpid())])\n",
    "\n",
    "callbacks = [TrainingMonitor(figPath, jsonPath=jsonPath),\n",
    "             LearningRateScheduler(lrs.poly_decay)]#we consider the attribute poly_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2bbc1",
   "metadata": {},
   "source": [
    "**Building the model**\n",
    "\n",
    "We consider the Stochastic Gradient Descent as regularization, the initial learning rate is $1e-2$ and the momentum is $0.9$. \n",
    "\n",
    "The input image size is $(32x32)$ and ten classes to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9ed5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model and the optimizer\n",
    "opt = SGD(lr=1e-2, momentum=0.9)\n",
    "model = MiniGoogLeNet.build(width=32, height=32, depth=3, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe02f6b",
   "metadata": {},
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a14d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Learning rate  0.001000\n",
      "Epoch 1/70\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 1.6973 - accuracy: 0.3685 - val_loss: 1.4402 - val_accuracy: 0.4905\n",
      "Epoch 2/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.3432 - accuracy: 0.5110 - val_loss: 1.1817 - val_accuracy: 0.5843\n",
      "Epoch 3/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.1820 - accuracy: 0.5761 - val_loss: 1.2319 - val_accuracy: 0.5716\n",
      "Epoch 4/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.0691 - accuracy: 0.6172 - val_loss: 0.9924 - val_accuracy: 0.6464\n",
      "Epoch 5/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.9891 - accuracy: 0.6510 - val_loss: 0.9134 - val_accuracy: 0.6776\n",
      "Epoch 6/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.9221 - accuracy: 0.6765 - val_loss: 1.0287 - val_accuracy: 0.6451\n",
      "Epoch 7/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.8655 - accuracy: 0.6944 - val_loss: 0.9902 - val_accuracy: 0.6611\n",
      "Epoch 8/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.8192 - accuracy: 0.7138 - val_loss: 0.8043 - val_accuracy: 0.7237\n",
      "Epoch 9/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.7804 - accuracy: 0.7279 - val_loss: 0.8380 - val_accuracy: 0.7076\n",
      "Epoch 10/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.7407 - accuracy: 0.7416 - val_loss: 0.8370 - val_accuracy: 0.7113\n",
      "Learning rate  0.000857\n",
      "Epoch 11/70\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.7076 - accuracy: 0.7547 - val_loss: 0.7168 - val_accuracy: 0.7510\n",
      "Epoch 12/70\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.6796 - accuracy: 0.7649 - val_loss: 0.7820 - val_accuracy: 0.7398\n",
      "Epoch 13/70\n",
      "781/781 [==============================] - 47s 61ms/step - loss: 0.6557 - accuracy: 0.7752 - val_loss: 0.7154 - val_accuracy: 0.7498\n",
      "Epoch 14/70\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.6343 - accuracy: 0.7821 - val_loss: 0.7478 - val_accuracy: 0.7501\n",
      "Epoch 15/70\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.6090 - accuracy: 0.7884 - val_loss: 0.6857 - val_accuracy: 0.7675\n",
      "Epoch 16/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5888 - accuracy: 0.7982 - val_loss: 0.8804 - val_accuracy: 0.7165\n",
      "Epoch 17/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5708 - accuracy: 0.8035 - val_loss: 0.6440 - val_accuracy: 0.7772\n",
      "Epoch 18/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5515 - accuracy: 0.8117 - val_loss: 0.6186 - val_accuracy: 0.7892\n",
      "Epoch 19/70\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.5330 - accuracy: 0.8181 - val_loss: 0.7401 - val_accuracy: 0.7523\n",
      "Epoch 20/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5179 - accuracy: 0.8230 - val_loss: 0.6525 - val_accuracy: 0.7850\n",
      "Learning rate  0.000714\n",
      "Epoch 21/70\n",
      "781/781 [==============================] - 48s 61ms/step - loss: 0.5079 - accuracy: 0.8252 - val_loss: 0.5560 - val_accuracy: 0.8129\n",
      "Epoch 22/70\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.4920 - accuracy: 0.8298 - val_loss: 0.5547 - val_accuracy: 0.8131\n",
      "Epoch 23/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4836 - accuracy: 0.8353 - val_loss: 0.5642 - val_accuracy: 0.8083\n",
      "Epoch 24/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4702 - accuracy: 0.8401 - val_loss: 0.6134 - val_accuracy: 0.7992\n",
      "Epoch 25/70\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.4619 - accuracy: 0.8413 - val_loss: 0.6478 - val_accuracy: 0.7898\n",
      "Epoch 26/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4463 - accuracy: 0.8476 - val_loss: 0.6284 - val_accuracy: 0.7957\n",
      "Epoch 27/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4362 - accuracy: 0.8496 - val_loss: 0.5778 - val_accuracy: 0.8089\n",
      "Epoch 28/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4286 - accuracy: 0.8511 - val_loss: 0.5791 - val_accuracy: 0.8120\n",
      "Epoch 29/70\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.4185 - accuracy: 0.8567 - val_loss: 0.5115 - val_accuracy: 0.8265\n",
      "Epoch 30/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4085 - accuracy: 0.8610 - val_loss: 0.4912 - val_accuracy: 0.8315\n",
      "Learning rate  0.000571\n",
      "Epoch 31/70\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.3971 - accuracy: 0.8653 - val_loss: 0.5379 - val_accuracy: 0.8253\n",
      "Epoch 32/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3915 - accuracy: 0.8669 - val_loss: 0.4967 - val_accuracy: 0.8360\n",
      "Epoch 33/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3844 - accuracy: 0.8684 - val_loss: 0.7021 - val_accuracy: 0.7857\n",
      "Epoch 34/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3746 - accuracy: 0.8721 - val_loss: 0.5318 - val_accuracy: 0.8238\n",
      "Epoch 35/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3687 - accuracy: 0.8745 - val_loss: 0.4823 - val_accuracy: 0.8408\n",
      "Epoch 36/70\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.3564 - accuracy: 0.8778 - val_loss: 0.4854 - val_accuracy: 0.8405\n",
      "Epoch 37/70\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.3524 - accuracy: 0.8814 - val_loss: 0.5015 - val_accuracy: 0.8353\n",
      "Epoch 38/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3432 - accuracy: 0.8829 - val_loss: 0.5192 - val_accuracy: 0.8325\n",
      "Epoch 39/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3355 - accuracy: 0.8856 - val_loss: 0.5616 - val_accuracy: 0.8215\n",
      "Epoch 40/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3301 - accuracy: 0.8879 - val_loss: 0.5715 - val_accuracy: 0.8156\n",
      "Learning rate  0.000429\n",
      "Epoch 41/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3238 - accuracy: 0.8893 - val_loss: 0.5091 - val_accuracy: 0.8317\n",
      "Epoch 42/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3141 - accuracy: 0.8927 - val_loss: 0.4460 - val_accuracy: 0.8508\n",
      "Epoch 43/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3128 - accuracy: 0.8912 - val_loss: 0.4550 - val_accuracy: 0.8513\n",
      "Epoch 44/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3092 - accuracy: 0.8939 - val_loss: 0.5130 - val_accuracy: 0.8339\n",
      "Epoch 45/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3031 - accuracy: 0.8961 - val_loss: 0.4899 - val_accuracy: 0.8400\n",
      "Epoch 46/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.2948 - accuracy: 0.8993 - val_loss: 0.4716 - val_accuracy: 0.8474\n",
      "Epoch 47/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.2912 - accuracy: 0.9004 - val_loss: 0.4859 - val_accuracy: 0.8434\n",
      "Epoch 48/70\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.2849 - accuracy: 0.9026 - val_loss: 0.4598 - val_accuracy: 0.8485\n",
      "Epoch 49/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2780 - accuracy: 0.9060 - val_loss: 0.5040 - val_accuracy: 0.8429\n",
      "Epoch 50/70\n",
      "781/781 [==============================] - 44s 57ms/step - loss: 0.2729 - accuracy: 0.9070 - val_loss: 0.4833 - val_accuracy: 0.8471\n",
      "Learning rate  0.000286\n",
      "Epoch 51/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2669 - accuracy: 0.9082 - val_loss: 0.4566 - val_accuracy: 0.8542\n",
      "Epoch 52/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2630 - accuracy: 0.9105 - val_loss: 0.4418 - val_accuracy: 0.8562\n",
      "Epoch 53/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2621 - accuracy: 0.9100 - val_loss: 0.4931 - val_accuracy: 0.8402\n",
      "Epoch 54/70\n",
      "781/781 [==============================] - 44s 57ms/step - loss: 0.2548 - accuracy: 0.9130 - val_loss: 0.4316 - val_accuracy: 0.8588\n",
      "Epoch 55/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2522 - accuracy: 0.9135 - val_loss: 0.4798 - val_accuracy: 0.8479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2479 - accuracy: 0.9152 - val_loss: 0.4569 - val_accuracy: 0.8530\n",
      "Epoch 57/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2420 - accuracy: 0.9174 - val_loss: 0.4812 - val_accuracy: 0.8481\n",
      "Epoch 58/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2380 - accuracy: 0.9196 - val_loss: 0.4534 - val_accuracy: 0.8523\n",
      "Epoch 59/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2374 - accuracy: 0.9188 - val_loss: 0.4432 - val_accuracy: 0.8552\n",
      "Epoch 60/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2327 - accuracy: 0.9209 - val_loss: 0.4317 - val_accuracy: 0.8609\n",
      "Learning rate  0.000143\n",
      "Epoch 61/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2261 - accuracy: 0.9224 - val_loss: 0.4404 - val_accuracy: 0.8587\n",
      "Epoch 62/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2271 - accuracy: 0.9244 - val_loss: 0.4676 - val_accuracy: 0.8513\n",
      "Epoch 63/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2224 - accuracy: 0.9252 - val_loss: 0.4470 - val_accuracy: 0.8557\n",
      "Epoch 64/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2204 - accuracy: 0.9264 - val_loss: 0.4324 - val_accuracy: 0.8600\n",
      "Epoch 65/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2175 - accuracy: 0.9272 - val_loss: 0.4409 - val_accuracy: 0.8575\n",
      "Epoch 66/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2148 - accuracy: 0.9278 - val_loss: 0.4327 - val_accuracy: 0.8610\n",
      "Epoch 67/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2123 - accuracy: 0.9285 - val_loss: 0.4398 - val_accuracy: 0.8602\n",
      "Epoch 68/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2116 - accuracy: 0.9284 - val_loss: 0.4242 - val_accuracy: 0.8626\n",
      "Epoch 69/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2066 - accuracy: 0.9312 - val_loss: 0.4198 - val_accuracy: 0.8650\n",
      "Epoch 70/70\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.2079 - accuracy: 0.9318 - val_loss: 0.4230 - val_accuracy: 0.8634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f426454df10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(aug.flow(X_train, y_train, batch_size=64), validation_data=(X_test, y_test),\n",
    "          steps_per_epoch=len(X_train) // 64, epochs=70, \n",
    "          callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edb8e0",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd9a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model on the disk\n",
    "model.save(\"output/minigooglenet_cifar10.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad983fd",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The model achieved a good result on the training set, accuracy of $0.93$, is the best results for this dataset in this training project. The result on the validation set shows accuracy of $0.86$, it's also a good result. The problem is the learning curves. There's a considerable gap between the training and validation set, that's indicates over-fit. This problem is recurrent, when the dataset in question is the CIFAR-10 dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
