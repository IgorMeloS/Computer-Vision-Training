{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d787f5f1",
   "metadata": {},
   "source": [
    "# Deeper GoogLeNet on Tiny ImageNet\n",
    "\n",
    "GoogLeNet was proposed in 2014 by [Szegedy et al.](https://arxiv.org/pdf/1409.4842.pdf) This Convolutional Neural Network (CNN) has introduced the concept of micro-architecture, it means, the model is composed by a certain number of micro-architecture, forming the macro-architecture.\n",
    "\n",
    "GoogLeNet introduced the inception module, it's composed by three convolution processing, including kernels size of $(1x1)$, $(3x3)$ and $(5x5)$. Each of them is parallel to the others during the running. The model was capable to increase the depth of the CNN, conserving a reasonable running time. At the end of the inception module, the model down sample all information to put into a feature map. If there's other inception module, other convolutions are performed, otherwise there's a maxpooling process and, the feature map is connected into the fully-connected layer, to make predictions. This model won the ImageNet Large-Scale Visual Recognition Challenge 2014.\n",
    "\n",
    "In this example, we deploy the full GoogLeNet model from scratch on the [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet). The dataset is a smaller version of ImageNet dataset, created for the Tiny ImageNet challenge. The Tiny ImageNet is composed by $200$ classes, each class contains $500$ images for the training set and $50$ for the validation and test set. All images have $64x64$ as size.\n",
    "\n",
    "To run the model, we utilize a DatasetGenerator, using HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e49b8",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89f5158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import tiny_imagenet_config as config\n",
    "from compvis.preprocessing import ImageToArrayPreprocessor\n",
    "from compvis.preprocessing import SimplePreprocessor\n",
    "from compvis.preprocessing import MeanPreprocessor\n",
    "from compvis.callbacks import EpochCheckPoint\n",
    "from compvis.callbacks import TrainingMonitor\n",
    "from compvis.io import HDF5DatasetGenerator\n",
    "from compvis.nn.cnns import DeeperGoogLeNet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15793134",
   "metadata": {},
   "source": [
    "## Loading the dataset and setting the data preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d537664",
   "metadata": {},
   "source": [
    "**Defining the data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3212a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data augumentation to improve the accuracy results\n",
    "aug = ImageDataGenerator(rotation_range=18, zoom_range=0.15, width_shift_range=0.2,\n",
    "                         height_shift_range=0.2, shear_range=0.15,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91a265",
   "metadata": {},
   "source": [
    "**Loading the RBG mean file**\n",
    "\n",
    "We normalize the pixels range according the RBG mean. It turns the pixels range around $0$ to each color channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7600bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the file for the RGB mean substraction\n",
    "means = json.loads(open(config.DATASET_MEAN).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e384db",
   "metadata": {},
   "source": [
    "**Image preprocessors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd67f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the image preprocessor to reduce, take mean and, convert to array\n",
    "sp = SimplePreprocessor(64, 64)\n",
    "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
    "iap = ImageToArrayPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175fe774",
   "metadata": {},
   "source": [
    "**Defining the Dataset Generator for the test and validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2b37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, 64, aug = aug,\n",
    "                                preprocessors = [sp, mp, iap],\n",
    "                                classes = config.NUM_CLASSES)\n",
    "valGen = HDF5DatasetGenerator(config.VAL_HDF5, 64,\n",
    "                              preprocessors = [sp, mp, iap],\n",
    "                              classes = config.NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3584ae",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "The training of this model is composed by three steps. Using the TraininigMonitor, we can follow the evolution of the learning curves. During the training the gap between the loss functions can increase or, these curves can stagnate. We also consider the EpochCheckPoint, saving each epoch during the training.\n",
    "\n",
    "To start the training, we set the variable modls as None, we then create and compile the model, with Adam regularizer, learning rate of $1e-3$. The training is made over $50$ epochs. Looking the graphic gave by the training monitoring, we can see that the model stagnates your learning from the epoch $39$.\n",
    "\n",
    "We restart the training from the epoch $39$, but we decrease the learning rate to $1e-4$. In this moment, the variable modls must be changed. Inside your project folder, there's an output folder, within this folder, we found the folder checkpoints, we select the path for the desired checkpoint, in this case $39$. Using load_model, we create the model and then, we drop down the learning rate. The model run $31$ times, totaling $70$ epochs. The learning curves stagnate from the epoch $63$.\n",
    "\n",
    "We restart the training from the epoch $63$, but we decrease the learning rate to $1e-5$. We train the model over $12$ epochs, totaling a model trained over $75$ epochs. In this stage, we also must change the variable modls.\n",
    "\n",
    "All metrics scores are stored into a json file, found in the output path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeaf2d5",
   "metadata": {},
   "source": [
    "**Creating and loading models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24afd169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading /home/igor/Documents/Artificial_Inteligence/Formation/Computer Vision Training/11 - GoogLeNet/output/checkpoints/epoch_63.hdf5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "## Creating and compiling the model\n",
    "# Checking if there is a callback\n",
    "#modl = None\n",
    "modl = 'path/to/project/output/checkpoints/epoch_63.hdf5'\n",
    "if modl is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = DeeperGoogLeNet.build(width = 64, height = 64, depth=3, classes = config.NUM_CLASSES,\n",
    "                                  reg = 0.0002)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer=opt, metrics = [\"accuracy\"])\n",
    "\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(modl))\n",
    "    # Updating the learning rate\n",
    "    model = load_model(modl)\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654dc7fd",
   "metadata": {},
   "source": [
    "**Defining the checkpoint folder path and the start epochs**\n",
    "\n",
    "The variable ckpt is the path for the checkpoints folder. The variable start is the start point of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41f3e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"/path/to/output/checkpoints\"\n",
    "start = 63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8496d",
   "metadata": {},
   "source": [
    "**Defining the EpochCheckPoint and TrainingMonitor callbacks**\n",
    "\n",
    "For this example, we save the checkpoint every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12521e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the set of Callbacks\n",
    "callbacks = [EpochCheckPoint(ckpt, every=1,startAt=start),\n",
    "             TrainingMonitor(config.FIG_PATH, jsonPath=config.JSON_PATH,\n",
    "                                             startAt=start)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936180c",
   "metadata": {},
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af6720f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1406 steps, validate for 156 steps\n",
      "Epoch 1/12\n",
      "1406/1406 [==============================] - 161s 114ms/step - loss: 1.8862 - accuracy: 0.6349 - val_loss: 2.9169 - val_accuracy: 0.4539\n",
      "Epoch 2/12\n",
      "1406/1406 [==============================] - 156s 111ms/step - loss: 1.8720 - accuracy: 0.6376 - val_loss: 2.8958 - val_accuracy: 0.4579\n",
      "Epoch 3/12\n",
      "1406/1406 [==============================] - 156s 111ms/step - loss: 1.8676 - accuracy: 0.6375 - val_loss: 2.8934 - val_accuracy: 0.4589\n",
      "Epoch 4/12\n",
      "1406/1406 [==============================] - 155s 111ms/step - loss: 1.8630 - accuracy: 0.6384 - val_loss: 2.8890 - val_accuracy: 0.4596\n",
      "Epoch 5/12\n",
      "1406/1406 [==============================] - 158s 112ms/step - loss: 1.8546 - accuracy: 0.6397 - val_loss: 2.8908 - val_accuracy: 0.4592\n",
      "Epoch 6/12\n",
      "1406/1406 [==============================] - 159s 113ms/step - loss: 1.8505 - accuracy: 0.6416 - val_loss: 2.8937 - val_accuracy: 0.4595\n",
      "Epoch 7/12\n",
      "1406/1406 [==============================] - 158s 112ms/step - loss: 1.8485 - accuracy: 0.6422 - val_loss: 2.8959 - val_accuracy: 0.4569\n",
      "Epoch 8/12\n",
      "1406/1406 [==============================] - 158s 112ms/step - loss: 1.8514 - accuracy: 0.6419 - val_loss: 2.8904 - val_accuracy: 0.4585\n",
      "Epoch 9/12\n",
      "1406/1406 [==============================] - 157s 112ms/step - loss: 1.8481 - accuracy: 0.6418 - val_loss: 2.8936 - val_accuracy: 0.4567\n",
      "Epoch 10/12\n",
      "1406/1406 [==============================] - 158s 113ms/step - loss: 1.8406 - accuracy: 0.6430 - val_loss: 2.8954 - val_accuracy: 0.4578\n",
      "Epoch 11/12\n",
      "1406/1406 [==============================] - 160s 114ms/step - loss: 1.8376 - accuracy: 0.6453 - val_loss: 2.9034 - val_accuracy: 0.4572\n",
      "Epoch 12/12\n",
      "1406/1406 [==============================] - 157s 112ms/step - loss: 1.8393 - accuracy: 0.6442 - val_loss: 2.9050 - val_accuracy: 0.4554\n"
     ]
    }
   ],
   "source": [
    "model.fit(trainGen.generator(), steps_per_epoch=trainGen.numImages // 64,\n",
    "                    validation_data=valGen.generator(), validation_steps=valGen.numImages // 64,\n",
    "                    epochs=12, max_queue_size=64 * 2, callbacks=callbacks, verbose=1)\n",
    "# close the databases\n",
    "trainGen.close()\n",
    "valGen.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f778d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After three step, we've trained the model, changing at each step the learning rate, always using the Adam regularizer. As we can note, the results for the accuracy is not so high and, the difference between the loss function is also high. For this dataset, there's other metric to evaluate the model, the error rate calculated as $ER = 1 - rank1$. You can found the graphics of the training process and the rank1 and 5 accuracy on the file evaluating.ipynb, in this folder. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
