{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36116cd",
   "metadata": {},
   "source": [
    "# Building HDF5 image dataset\n",
    "\n",
    "In this notebook, we create a series of datasets for the train, validation and test sets, using HDF5 file. We consider the [Dogs vs Cats Kaggle dataset](https://www.kaggle.com/c/dogs-vs-cats/data). The dataset is composed by $25000$ images on the training set and $12000$ for the test set. We also extract the mean of the colors channels (RGB). We utilize a configuration file to set our directories (input/output) paths, the number of classes and the portion of images to the validation and test set.\n",
    "\n",
    "**Note**: For this example, we just consider the train set from the Dogs vs Cats dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f079e8",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab69fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import dogs_vs_cats_config as config\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "#compvis module\n",
    "from compvis.preprocessing import ResizeAR\n",
    "from compvis.io import HDF5DatasetWriter\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ce462",
   "metadata": {},
   "source": [
    "## Creating the list of images and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12044356",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPaths = list(paths.list_images(config.IMAGES_PATH)) # list of paths\n",
    "trainLabels = [p.split(os.path.sep)[-1].split(\".\")[0] for p in trainPaths] # list of labels\n",
    "# we split the path to obtain the labels dog and cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dacd83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312ee7b",
   "metadata": {},
   "source": [
    "## Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06c2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the labels into the intergers\n",
    "le = LabelEncoder()\n",
    "trainLabels = le.fit_transform(trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40fcca1",
   "metadata": {},
   "source": [
    "## Splitting the list of Paths into train, validation and test set\n",
    "\n",
    "We split the dataset into train, test and validation. This process is made in two step. In the first step we define the test set, the size for it is $2500$. Note, the length of the trainPaths was changed into $22500$. In the second step, we define the validation set with $2500$, the final length of the train set is $20000$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90c1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training split, a validation split, and a testing split.\n",
    "split = train_test_split(trainPaths, trainLabels, test_size = config.NUM_TEST_IMAGES,\n",
    "                         stratify = trainLabels, random_state = 42)\n",
    "(trainPaths, testPaths, trainLabels, testLabels) = split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d408ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c468dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform another stratified sampling, this time to build the\n",
    "# validation data\n",
    "split = train_test_split(trainPaths, trainLabels, test_size = config.NUM_VAL_IMAGES,\n",
    "                         stratify = trainLabels, random_state = 42)\n",
    "(trainPaths, valPaths, trainLabels, valLabels) = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f81643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5ed77",
   "metadata": {},
   "source": [
    "## Building the train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e5723ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a list pairing the training, validation, and testing\n",
    "# image paths along with their corresponding labels and output HDF5\n",
    "# files\n",
    "datasets = [(\"train\", trainPaths, trainLabels, config.TRAIN_HDF5),\n",
    "            (\"val\", valPaths, valLabels, config.VAL_HDF5),\n",
    "            (\"test\", testPaths, testLabels, config.TEST_HDF5)] # this will be used with in HDF5DatasetWriter\n",
    "#(name of dataset, The path for the images, list  of label, path to save the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a45d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the image preprocessor and the list of channels RGB\n",
    "aap = ResizeAR(256, 256)\n",
    "(R, G, B) = ([], [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef7e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building /home/igor/Documents/Artificial_Inteligence/Datasets/Cats and dogs /hdf5/train.hdf5...\n",
      "[INFO] building /home/igor/Documents/Artificial_Inteligence/Datasets/Cats and dogs /hdf5/val.hdf5...\n",
      "[INFO] building /home/igor/Documents/Artificial_Inteligence/Datasets/Cats and dogs /hdf5/test.hdf5...\n"
     ]
    }
   ],
   "source": [
    "for (dType, paths, labels, outputPath) in datasets:\n",
    "    # Creating the HDF5 file\n",
    "    print(\"[INFO] building {}...\".format(outputPath))\n",
    "    # the arguments for the dataset writer are a tuple with the total of images and the total of features\n",
    "    writer = HDF5DatasetWriter((len(paths), 256, 256, 3), outputPath)\n",
    "    # Loop over the images Paths\n",
    "    for (i, (path, label)) in enumerate(zip(paths, labels)):\n",
    "        image = cv2.imread(path)\n",
    "        image = aap.preprocess(image)\n",
    "        # if we are building the training dataset, then compute the\n",
    "        # mean of each channel in the image, then update the\n",
    "        # respective lists\n",
    "        if dType == \"train\":\n",
    "            (b, g, r) = cv2.mean(image)[:3]\n",
    "            R.append(r)\n",
    "            G.append(g)\n",
    "            B.append(b)\n",
    "        #The attribute add, write the features and the list of labels in the dataset\n",
    "        writer.add([image], [label])\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096ab69",
   "metadata": {},
   "source": [
    "## Saving the json file with the means of RGB channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76c2046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing means...\n"
     ]
    }
   ],
   "source": [
    "# construct a dictionary of averages, then serialize the means to a\n",
    "# JSON file\n",
    "print(\"[INFO] serializing means...\" )\n",
    "D = {\"R\" : np.mean(R), \"G\" : np.mean(G), \"B\" : np.mean(B)}\n",
    "f = open(config.DATASET_MEAN, \"w\")\n",
    "f.write(json.dumps(D))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
